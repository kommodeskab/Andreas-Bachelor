# @package _global_
defaults:
  - override /trainer: gpu
  - override /logger: default
  - override /callbacks: diffusion
  - override /model: diffusion_model
  - override /data: none

project_name: 'diffusion'
task_name: 'test'

callbacks:
  swa:
    _target_: pytorch_lightning.callbacks.StochasticWeightAveraging
    swa_lrs: 1e-2
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: 'val_loss'
    patience: 30
    mode: 'min'

trainer:
  log_every_n_steps: 5
  max_epochs: -1
  check_val_every_n_epoch: 10
  val_check_interval: 1.0
  gradient_clip_val: 1.0

data:
  _target_: src.data_modules.BaseDM
  dataset:
    _target_: src.dataset.AFHQDataset
    split: 'cat'
    img_size: 32
    train: True
    augment: True
    size_multiplier: 5
  val_dataset:
    _target_: src.dataset.AFHQDataset
    split: 'cat'
    img_size: 32
    train: False
  batch_size: 64
  num_workers: 4

model:
  _target_: src.lightning_modules.DDPM
  prediction_type: 'epsilon'
  num_train_timesteps: 100
  lr_patience: 10
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: True
    lr: 5e-5

  model:
    _target_: src.networks.unets.UNet2D
    dropout: 0.1
    sample_size: [32, 32]
    in_channels: 3
    out_channels: 3
    block_out_channels: [128, 192, 256]
    down_block_types: ["DownBlock2D", "AttnDownBlock2D", "AttnDownBlock2D"]
    up_block_types: ["AttnUpBlock2D", "AttnUpBlock2D", "UpBlock2D"]


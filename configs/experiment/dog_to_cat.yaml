# @package _global_
defaults:
  - override /trainer: gpu
  - override /data: dog_to_cat
  - override /callbacks: schrodinger_images
  - override /logger: default
  - override /model: big_images

project_name: 'dog_to_cat'
task_name: 'test'

trainer:
  val_check_interval: 5000
  log_every_n_steps: 100
  max_epochs: 200
  limit_val_batches: 1

callbacks:
  fid:
    _target_: src.callbacks.CalculateFID
    num_samples: 1000
    ema_scope: True

model:
  _target_: src.lightning_modules.TRDSB
  max_gamma: 0.01
  min_gamma: 0.001
  num_steps: 100
  T: 1.0
  # patience: 2
  max_iterations: 20001
  initial_forward_sampling: "ornstein_1"
  max_norm: 1.0

  forward_model:
    dropout: 0.2
    sample_size: [32, 32]
    in_channels: 3
    out_channels: 3
    layers_per_block: 2
    block_out_channels: [64, 64, 128, 128]
    down_block_types: ["DownBlock2D", "AttnDownBlock2D", "AttnDownBlock2D", "AttnDownBlock2D"]
    up_block_types: ["AttnUpBlock2D", "AttnUpBlock2D", "AttnUpBlock2D", "UpBlock2D"]
  
  backward_model:
    dropout: 0.2
    sample_size: [32, 32]
    in_channels: 3
    out_channels: 3
    layers_per_block: 2
    block_out_channels: [64, 64, 128, 128]
    down_block_types: ["DownBlock2D", "AttnDownBlock2D", "AttnDownBlock2D", "AttnDownBlock2D"]
    up_block_types: ["AttnUpBlock2D", "AttnUpBlock2D", "AttnUpBlock2D", "UpBlock2D"]

  optimizer:
    _target_: torch.optim.AdamW
    _partial_: True
    lr: 1e-4

  scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
    _partial_: True
    T_0: 1000
    T_mult: 2
    eta_min: 1e-6

data:
  cache_num_iters: 200
  batch_size: 128
  num_workers: 4
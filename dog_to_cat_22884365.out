Loaded dependency [python3/3.12.4]: sqlite3/3.46.0
Loaded dependency [python3/3.12.4]: gcc/12.4.0-binutils-2.42
Loaded module: python3/3.12.4

Loading python3/3.12.4
  Loading requirement: sqlite3/3.46.0 gcc/12.4.0-binutils-2.42
Seed set to 42
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: kommodeskab (kommodeskab-danmarks-tekniske-universitet-dtu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in logs/wandb/run-20241020_204149-201024204147
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run test
wandb: â­ï¸ View project at https://wandb.ai/kommodeskab-danmarks-tekniske-universitet-dtu/dog_to_cat
wandb: ğŸš€ View run at https://wandb.ai/kommodeskab-danmarks-tekniske-universitet-dtu/dog_to_cat/runs/201024204147
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Config:

trainer:
  max_epochs: 200
  accelerator: gpu
  log_every_n_steps: 40
  reload_dataloaders_every_n_epochs: 1
  val_check_interval: 1000
  num_sanity_val_steps: 0
  profiler: simple
  limit_val_batches: 5
  fast_dev_run: false
data:
  _target_: src.data_modules.BaseDSBDM
  start_dataset:
    _target_: src.dataset.AFHQ
    split: dog
    img_size: 32
  end_dataset:
    _target_: src.dataset.AFHQ
    split: cat
    img_size: 32
  batch_size: 64
  cache_num_iters: 400
  num_workers: 4
  train_val_split: 0.9
model:
  _target_: src.lightning_modules.TRDSB
  forward_model:
    _target_: src.networks.unets.UNet2D
    sample_size:
    - 32
    - 32
    in_channels: 3
    out_channels: 3
    layers_per_block: 2
    down_block_types:
    - DownBlock2D
    - AttnDownBlock2D
    - AttnDownBlock2D
    - AttnDownBlock2D
    up_block_types:
    - AttnUpBlock2D
    - AttnUpBlock2D
    - AttnUpBlock2D
    - UpBlock2D
    block_out_channels:
    - 64
    - 128
    - 256
    - 512
    dropout: 0.1
  backward_model:
    _target_: src.networks.unets.UNet2D
    sample_size:
    - 32
    - 32
    in_channels: 3
    out_channels: 3
    layers_per_block: 2
    down_block_types:
    - DownBlock2D
    - AttnDownBlock2D
    - AttnDownBlock2D
    - AttnDownBlock2D
    up_block_types:
    - AttnUpBlock2D
    - AttnUpBlock2D
    - AttnUpBlock2D
    - UpBlock2D
    block_out_channels:
    - 64
    - 128
    - 256
    - 512
    dropout: 0.1
  patience: 10
  max_gamma: 0.1
  min_gamma: 0.01
  num_steps: 50
  T: 1.5
  max_iterations: 30000
  initial_forward_sampling: diffuse
  max_norm: 3.0
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 0.0001
  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    mode: min
    factor: 0.5
    patience: 4
callbacks:
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step
  richprogressbar:
    _target_: pytorch_lightning.callbacks.RichProgressBar
  plotgammaschedule:
    _target_: src.callbacks.PlotGammaScheduleCB
  model_summary:
    _target_: pytorch_lightning.callbacks.RichModelSummary
    max_depth: 2
  plot_images:
    _target_: src.callbacks.PlotImagesCB
    ema_scope: true
  image_grid:
    _target_: src.callbacks.PlotImageSamplesCB
  marginal_distributions:
    _target_: src.callbacks.MarginalDistributionsImagesCB
  initial_diffusion:
    _target_: src.callbacks.TestInitialDiffusionCB
    num_rows: 5
  fid:
    _target_: src.callbacks.CalculateFID
    num_samples: 1000
    ema_scope: true
logger:
  save_dir: logs
  offline: false
  log_model: false
project_name: dog_to_cat
task_name: test
seed: 42
compile: true

Instantiating model and datamodule..
Compiling model..
Setting up logger..
Instantiating callbacks..
Setting up trainer..
Beginning training..
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                          â”ƒ Type              â”ƒ Params â”ƒ Mode  â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ forward_model                 â”‚ UNet2D            â”‚ 63.5 M â”‚ train â”‚
â”‚ 1  â”‚ forward_model.conv_in         â”‚ Conv2d            â”‚  1.8 K â”‚ train â”‚
â”‚ 2  â”‚ forward_model.time_proj       â”‚ Timesteps         â”‚      0 â”‚ train â”‚
â”‚ 3  â”‚ forward_model.time_embedding  â”‚ TimestepEmbedding â”‚ 82.4 K â”‚ train â”‚
â”‚ 4  â”‚ forward_model.down_blocks     â”‚ ModuleList        â”‚ 15.2 M â”‚ train â”‚
â”‚ 5  â”‚ forward_model.up_blocks       â”‚ ModuleList        â”‚ 37.4 M â”‚ train â”‚
â”‚ 6  â”‚ forward_model.mid_block       â”‚ UNetMidBlock2D    â”‚ 10.8 M â”‚ train â”‚
â”‚ 7  â”‚ forward_model.conv_norm_out   â”‚ GroupNorm         â”‚    128 â”‚ train â”‚
â”‚ 8  â”‚ forward_model.conv_act        â”‚ SiLU              â”‚      0 â”‚ train â”‚
â”‚ 9  â”‚ forward_model.conv_out        â”‚ Conv2d            â”‚  1.7 K â”‚ train â”‚
â”‚ 10 â”‚ backward_model                â”‚ UNet2D            â”‚ 63.5 M â”‚ train â”‚
â”‚ 11 â”‚ backward_model.conv_in        â”‚ Conv2d            â”‚  1.8 K â”‚ train â”‚
â”‚ 12 â”‚ backward_model.time_proj      â”‚ Timesteps         â”‚      0 â”‚ train â”‚
â”‚ 13 â”‚ backward_model.time_embedding â”‚ TimestepEmbedding â”‚ 82.4 K â”‚ train â”‚
â”‚ 14 â”‚ backward_model.down_blocks    â”‚ ModuleList        â”‚ 15.2 M â”‚ train â”‚
â”‚ 15 â”‚ backward_model.up_blocks      â”‚ ModuleList        â”‚ 37.4 M â”‚ train â”‚
â”‚ 16 â”‚ backward_model.mid_block      â”‚ UNetMidBlock2D    â”‚ 10.8 M â”‚ train â”‚
â”‚ 17 â”‚ backward_model.conv_norm_out  â”‚ GroupNorm         â”‚    128 â”‚ train â”‚
â”‚ 18 â”‚ backward_model.conv_act       â”‚ SiLU              â”‚      0 â”‚ train â”‚
â”‚ 19 â”‚ backward_model.conv_out       â”‚ Conv2d            â”‚  1.7 K â”‚ train â”‚
â”‚ 20 â”‚ mse                           â”‚ MSELoss           â”‚      0 â”‚ train â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 126 M                                                         
Non-trainable params: 0                                                         
Total params: 126 M                                                             
Total estimated model params size (MB): 507                                     
Modules in train mode: 704                                                      
Modules in eval mode: 0                                                         

Detected KeyboardInterrupt, attempting graceful shutdown ...
Epoch 0/199 â”â”â”â”â”â”          11000/26400 0:38:20 â€¢       5.43it/s v_num: 4147    
                                        0:47:16                  backward_modelâ€¦
                                                                 0.235          
                                                                 iteration_1/baâ€¦
                                                                 0.060          
                                                                 iteration_1/baâ€¦
                                                                 0.183          
wandb: Encountered an error while tearing down the service manager: [Errno 32] Broken pipe

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 22884365: <dog_to_cat> in cluster <dcc> Exited

Job <dog_to_cat> was submitted from host <gbarlogin1> by user <s214630> in cluster <dcc> at Sun Oct 20 20:41:29 2024
Job was executed on host(s) <4*n-62-20-15>, in queue <gpuv100>, as user <s214630> in cluster <dcc> at Sun Oct 20 20:41:29 2024
</zhome/79/c/169097> was used as the home directory.
</work3/s214630/Andreas-Bachelor> was used as the working directory.
Started at Sun Oct 20 20:41:29 2024
Terminated at Sun Oct 20 21:20:32 2024
Results reported at Sun Oct 20 21:20:32 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh

#BSUB -J dog_to_cat
#BSUB -q gpuv100
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -n 4
#BSUB -R "rusage[mem=16G]"
#BSUB -R "span[hosts=1]"
#BSUB -W 24:00


# load a scipy module
module load python3/3.12.4

# activate the virtual environment
source .venv/bin/activate

python3 train.py +experiment=dog_to_cat logger.offline=False trainer.log_every_n_steps=40
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   2615.00 sec.
    Max Memory :                                 2275 MB
    Average Memory :                             1859.55 MB
    Total Requested Memory :                     65536.00 MB
    Delta Memory :                               63261.00 MB
    Max Swap :                                   -
    Max Processes :                              23
    Max Threads :                                90
    Run time :                                   2455 sec.
    Turnaround time :                            2343 sec.

The output (if any) is above this job summary.

